# ğŸ¤– Chat AI with Ollama - Offline AI Chat Application

A beautiful, privacy-focused chat application powered by **Ollama** - run powerful AI models completely offline on your local machine!

![Version](https://img.shields.io/badge/version-2.0.0-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Python](https://img.shields.io/badge/python-3.7+-brightgreen)

## âœ¨ Features

- ğŸ”’ **100% Private & Offline** - All AI processing happens locally, no data sent to cloud
- ğŸ“¥ **Easy Model Management** - Download, use, and delete AI models from the dashboard
- ğŸ“ **Custom Models Directory** - Models stored in `~/script_files/vovagpt/data/models/` for better organization
- ğŸ’¬ **Multiple Chats** - Create and manage unlimited conversations
- ğŸ‘¥ **User System** - Root admin can create and manage multiple users
- ğŸ¨ **Beautiful UI** - Modern, ChatGPT-like interface
- ğŸ“Š **Real-time Progress** - Watch model downloads with live progress bars
- ğŸš€ **Fast & Responsive** - Local processing with no network latency
- ğŸ”„ **Model Switching** - Use different models for different chats

## ğŸ¯ Quick Start

### 1. Install Ollama
```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama
```

### 2. Install Dependencies
```bash
cd app
pip install -r requirements.txt
```

### 3. Run the App
```bash
# Easiest way - automated start script
./start.sh

# Or manually
ollama serve          # Terminal 1
python app.py         # Terminal 2
```

### 4. Access & Setup
1. Open browser: **http://localhost:5000**
2. Register root user (admin)
3. Download your first AI model
4. Create a chat and start talking!

## ğŸ“¦ Available Models

Popular models included (download from dashboard):

| Model | Size | Best For |
|-------|------|----------|
| **llama3.2** | 2GB | General use, fast |
| **qwen2.5** | 4.7GB | Coding, multilingual |
| **deepseek-r1** | 4.7GB | Reasoning tasks |
| **mistral** | 4.1GB | Balanced performance |
| **phi4** | 7.9GB | Compact but powerful |
| **codellama** | 3.8GB | Code generation |

## ğŸ“¸ Screenshots

### Dashboard - Model Management
- View downloaded models
- Download new models with progress
- Manage your chats

### Chat Interface
- Clean, modern design
- Real-time responses
- Conversation history

### Settings
- Choose default model
- Manage preferences

## ğŸ—ï¸ Architecture

```
Browser (UI) 
    â†•ï¸
Flask App (Backend)
    â†•ï¸
Ollama (AI Server)
    â†•ï¸
Local Models (Storage)
```

**Tech Stack:**
- **Frontend**: HTML, CSS, JavaScript (vanilla)
- **Backend**: Flask, Python
- **AI**: Ollama (local models)
- **Storage**: JSON files (portable)

## ğŸ“š Documentation

- **[Quick Start Guide](QUICK_START.md)** - Get up and running in 5 minutes
- **[Setup Documentation](OLLAMA_SETUP.md)** - Detailed installation and configuration
- **[Custom Models Directory](CUSTOM_MODELS_DIR.md)** - How models are stored in custom location
- **[Migration Summary](MIGRATION_SUMMARY.md)** - What changed from cloud APIs to Ollama
- **[Architecture](ARCHITECTURE.md)** - Technical architecture and design patterns

## ğŸ”§ Configuration

### Environment Variables
```bash
# Optional - defaults shown
export OLLAMA_HOST=http://localhost:11434
export SECRET_KEY=your-secret-key-here
export VERSION=2.0.0
```

### Docker Deployment
```bash
cd app
docker-compose up -d
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Q: Models won't download?**
```bash
# Ensure Ollama is running
ollama serve
```

**Q: Chat errors?**
```bash
# Verify model is downloaded
ollama list
```

**Q: Can't connect?**
```bash
# Check Ollama is accessible
curl http://localhost:11434/api/tags
```

See [OLLAMA_SETUP.md](OLLAMA_SETUP.md) for detailed troubleshooting.

## ğŸ’¡ Usage Tips

1. **Start Small**: Download llama3.2 (2GB) first for quick testing
2. **Storage**: Ensure sufficient disk space (models: 2-40GB each)
3. **Performance**: 8GB+ RAM recommended, GPU optional but helpful
4. **Offline**: After models downloaded, works 100% offline

## ğŸŒŸ Key Benefits

| Feature | Cloud APIs | Ollama (This App) |
|---------|-----------|-------------------|
| **Privacy** | âŒ Data sent to servers | âœ… 100% local |
| **Cost** | ğŸ’° Pay per use | âœ… Free forever |
| **Offline** | âŒ Internet required | âœ… Works offline |
| **Speed** | â±ï¸ Network dependent | âš¡ Local speed |
| **Control** | âŒ Limited | âœ… Full control |

## ğŸ“ Project Structure

```
vovagpt_src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                 # Main application
â”‚   â”œâ”€â”€ requirements.txt       # Dependencies
â”‚   â”œâ”€â”€ start.sh              # Quick start script
â”‚   â”œâ”€â”€ docker-compose.yml    # Docker config
â”‚   â””â”€â”€ templates/            # HTML templates
â”‚       â”œâ”€â”€ dashboard.html    # Model management
â”‚       â”œâ”€â”€ chat.html         # Chat interface
â”‚       â””â”€â”€ settings.html     # User settings
â”‚
â”œâ”€â”€ QUICK_START.md           # Quick start guide
â”œâ”€â”€ OLLAMA_SETUP.md          # Detailed setup
â”œâ”€â”€ MIGRATION_SUMMARY.md     # Migration notes
â”œâ”€â”€ ARCHITECTURE.md          # Technical docs
â””â”€â”€ README.md                # This file
```

## ğŸ¤ Contributing

This is a personal/educational project. Feel free to:
- Report issues
- Suggest features
- Fork and modify for your needs

## ğŸ“„ License

This project is provided as-is for personal and educational use.

## ğŸ™ Credits

- **Ollama** - Local AI model server
- **Flask** - Web framework
- **Open source AI models** - Llama, Qwen, DeepSeek, Mistral, etc.

## ğŸ“ Support

- **Ollama Issues**: [ollama.com](https://ollama.com) | [GitHub](https://github.com/ollama/ollama)
- **App Issues**: Check [OLLAMA_SETUP.md](OLLAMA_SETUP.md) troubleshooting section

---

<div align="center">

### â­ Enjoy your private, offline AI chat experience! â­

**Made with â¤ï¸ for privacy-conscious AI users**

[Quick Start](QUICK_START.md) â€¢ [Documentation](OLLAMA_SETUP.md) â€¢ [Architecture](ARCHITECTURE.md)

</div>

