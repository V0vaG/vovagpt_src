version: '3.8'

services:
  chat_app:
    build: .
    container_name: chat_ai_app
    network_mode: host  # Required to access Ollama on localhost
    environment:
      - SECRET_KEY=${SECRET_KEY:-your_secret_key_change_in_production}
      - VERSION=${VERSION:-2.0.0}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://localhost:11434}
      - OLLAMA_MODELS=/root/script_files/vovagpt/data/models
    volumes:
      - ./data:/root/script_files/vovagpt/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: chat_ai_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - chat_app
    restart: unless-stopped
    network_mode: host  # Required for host network access

