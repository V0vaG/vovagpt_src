# ğŸ—ï¸ Architecture - Chat AI with Ollama

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User's Browser                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Dashboard  â”‚  â”‚  Chat View   â”‚  â”‚   Settings       â”‚    â”‚
â”‚  â”‚            â”‚  â”‚              â”‚  â”‚                  â”‚    â”‚
â”‚  â”‚ â€¢ Models   â”‚  â”‚ â€¢ Messages   â”‚  â”‚ â€¢ Preferences    â”‚    â”‚
â”‚  â”‚ â€¢ Download â”‚  â”‚ â€¢ Send/Recv  â”‚  â”‚ â€¢ Model Select   â”‚    â”‚
â”‚  â”‚ â€¢ Delete   â”‚  â”‚ â€¢ History    â”‚  â”‚                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTP/SSE
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flask Application                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Routes & Logic                    â”‚   â”‚
â”‚  â”‚  â€¢ /dashboard    - Model management UI              â”‚   â”‚
â”‚  â”‚  â€¢ /chat/<id>    - Chat interface                   â”‚   â”‚
â”‚  â”‚  â€¢ /model/download/<name> - Stream download (SSE)   â”‚   â”‚
â”‚  â”‚  â€¢ /model/delete/<name>   - Remove model            â”‚   â”‚
â”‚  â”‚  â€¢ /models/list  - Get available models             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Ollama Integration Layer                â”‚   â”‚
â”‚  â”‚  â€¢ get_ollama_models()          - List installed    â”‚   â”‚
â”‚  â”‚  â€¢ get_available_ollama_models() - List available   â”‚   â”‚
â”‚  â”‚  â€¢ download_ollama_model()      - Pull with progressâ”‚   â”‚
â”‚  â”‚  â€¢ delete_ollama_model()        - Remove model      â”‚   â”‚
â”‚  â”‚  â€¢ get_ai_response()            - Chat with model   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                Data Persistence                      â”‚   â”‚
â”‚  â”‚  ~/script_files/chat/data/                          â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ users.json  (User accounts)                  â”‚   â”‚
â”‚  â”‚    â””â”€â”€ chats.json  (Chat history)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTP REST API
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ollama Server                             â”‚
â”‚                  (localhost:11434)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  Ollama API                          â”‚   â”‚
â”‚  â”‚  â€¢ /api/tags     - List models                      â”‚   â”‚
â”‚  â”‚  â€¢ /api/pull     - Download model (streaming)       â”‚   â”‚
â”‚  â”‚  â€¢ /api/delete   - Remove model                     â”‚   â”‚
â”‚  â”‚  â€¢ /api/chat     - Chat completion                  â”‚   â”‚
â”‚  â”‚  â€¢ /api/show     - Model details                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Local AI Models Storage                 â”‚   â”‚
â”‚  â”‚  ~/.ollama/models/                                   â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ llama3.2/                                     â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ qwen2.5/                                      â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ deepseek-r1/                                  â”‚   â”‚
â”‚  â”‚    â””â”€â”€ ... (other models)                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow

### 1. Model Download Flow
```
User clicks "Download" 
    â†“
JavaScript initiates EventSource connection
    â†“
Flask route: /model/download/<name>
    â†“
Calls: download_ollama_model(name)
    â†“
HTTP POST to Ollama: /api/pull (streaming)
    â†“
Server-Sent Events stream progress to browser
    â†“
Progress bar updates in real-time
    â†“
On complete: Page reloads, model appears in "Downloaded"
```

### 2. Chat Message Flow
```
User types message and clicks Send
    â†“
JavaScript: POST /chat/<id>/message
    â†“
Flask: Receives message, adds to chat history
    â†“
Prepares messages array for Ollama
    â†“
Calls: get_ai_response(messages, model)
    â†“
HTTP POST to Ollama: /api/chat
    â†“
Ollama processes with local AI model
    â†“
Response returned to Flask
    â†“
Saved to chat history (chats.json)
    â†“
JSON response to browser
    â†“
Message displayed in chat UI
```

### 3. Model Management Flow
```
Dashboard loads
    â†“
Flask calls: get_ollama_models()
    â†“
HTTP GET to Ollama: /api/tags
    â†“
Returns list of installed models
    â†“
Also gets: get_available_ollama_models()
    â†“
Compares lists to show only uninstalled models
    â†“
Renders dashboard with both lists
```

## ğŸ—‚ï¸ File Structure

```
app/
â”œâ”€â”€ app.py                          # Main Flask application
â”‚   â”œâ”€â”€ Ollama Functions            # Model management
â”‚   â”œâ”€â”€ Routes                      # HTTP endpoints
â”‚   â”œâ”€â”€ User Management            # Auth & sessions
â”‚   â””â”€â”€ Chat Logic                 # Conversation handling
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ dashboard.html             # Model management UI
â”‚   â”‚   â”œâ”€â”€ Downloaded Models      # Grid of installed models
â”‚   â”‚   â”œâ”€â”€ Available Models       # Grid with download buttons
â”‚   â”‚   â””â”€â”€ Chat List              # User's conversations
â”‚   â”‚
â”‚   â”œâ”€â”€ chat.html                  # Chat interface
â”‚   â”‚   â”œâ”€â”€ Message Display        # Conversation view
â”‚   â”‚   â”œâ”€â”€ Input Area             # Send messages
â”‚   â”‚   â””â”€â”€ Model Display          # Shows active model
â”‚   â”‚
â”‚   â”œâ”€â”€ settings.html              # User preferences
â”‚   â””â”€â”€ login.html / register*.html # Authentication
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ docker-compose.yml            # Container orchestration
â”œâ”€â”€ Dockerfile                    # Container image
â””â”€â”€ start.sh                      # Quick start script
```

## ğŸ” Security Architecture

```
Session Management:
â”œâ”€â”€ Flask sessions (server-side)
â”œâ”€â”€ Secure password hashing (pbkdf2:sha256)
â””â”€â”€ User isolation (chats separated by user)

Data Privacy:
â”œâ”€â”€ All data stored locally
â”œâ”€â”€ No external API calls
â”œâ”€â”€ Ollama runs on localhost
â””â”€â”€ Optional: Docker network isolation
```

## ğŸŒŠ Request/Response Patterns

### Standard HTTP Requests
```
Browser â†’ Flask â†’ Ollama â†’ Flask â†’ Browser
  [Request] â†’ [Process] â†’ [Response]
```

### Server-Sent Events (Model Download)
```
Browser â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Flask â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ollama
        [Event Stream with progress updates]
```

### WebSocket Alternative (Not Used)
```
We use SSE instead of WebSocket because:
âœ… Simpler implementation
âœ… Built-in browser support (EventSource)
âœ… One-way communication sufficient
âœ… Automatic reconnection
```

## ğŸ”§ Technology Stack

### Frontend
- **HTML5** - Structure
- **CSS3** - Styling (no frameworks, custom design)
- **Vanilla JavaScript** - Interactivity
- **EventSource API** - Real-time updates
- **Fetch API** - AJAX requests

### Backend
- **Flask** - Web framework
- **Python 3.7+** - Programming language
- **Werkzeug** - WSGI utilities & security
- **Requests** - HTTP client for Ollama API

### AI Layer
- **Ollama** - Local AI model server
- **REST API** - Communication protocol
- **Streaming** - Real-time responses

### Data Storage
- **JSON Files** - User data & chat history
- **File System** - Local storage
- **No Database** - Simple, portable

### Deployment
- **Gunicorn** - WSGI HTTP Server
- **Docker** - Containerization
- **Nginx** - Reverse proxy (optional)

## ğŸ¯ Design Patterns

### 1. **Model-View-Controller (MVC)**
- **Model**: JSON files (users.json, chats.json)
- **View**: HTML templates (Jinja2)
- **Controller**: Flask routes (app.py)

### 2. **Repository Pattern**
```python
def load_users() / save_users()
def load_chats() / save_chats()
```

### 3. **Decorator Pattern**
```python
@login_required
def protected_route():
    pass
```

### 4. **Streaming Pattern**
```python
def generate():
    for chunk in stream:
        yield chunk
return Response(generate(), mimetype='text/event-stream')
```

## ğŸ“Š Performance Considerations

### Optimizations
- âœ… Async model downloads (non-blocking)
- âœ… Streaming responses (progressive loading)
- âœ… Client-side state management
- âœ… Minimal database queries (JSON files)
- âœ… Local processing (no network latency)

### Scalability
- **Users**: Suitable for 1-50 users
- **Chats**: Handles 1000s of conversations
- **Models**: Limited by disk space
- **Concurrent**: Flask handles multiple requests

## ğŸ”„ State Management

### Server State
```
Session:
â”œâ”€â”€ user_id (username)
â””â”€â”€ is_root (boolean)

Files:
â”œâ”€â”€ users.json (user accounts)
â””â”€â”€ chats.json (conversations)
```

### Client State
```
JavaScript Variables:
â”œâ”€â”€ Current chat ID
â”œâ”€â”€ Model selection
â”œâ”€â”€ Download progress
â””â”€â”€ UI state (modals, etc.)
```

## ğŸš€ Deployment Options

### Option 1: Standalone
```
Ollama (localhost:11434) â† Flask (localhost:5000) â† Users
```

### Option 2: Docker (Host Network)
```
Ollama (host:11434) â† Flask (container:5000) â† Nginx (80/443) â† Users
```

### Option 3: Remote Ollama
```
Ollama (server:11434) â† Flask (app:5000) â† Users
         [LAN/VPN]
```

---

**This architecture provides a balance of simplicity, performance, and privacy for local AI chat!** ğŸ¯

