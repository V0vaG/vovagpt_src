# ğŸš€ Quick Start Guide - Chat AI with Ollama

## ğŸ“¥ Installation (One-time)

### Step 1: Install Ollama
```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama
```

### Step 2: Install Python Dependencies
```bash
cd app
pip install -r requirements.txt
```

## ğŸƒ Running the App

### Easiest Way - Use Start Script:
```bash
cd app
./start.sh
```

### Manual Way:
```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start App
cd app
python app.py
```

Then open: **http://localhost:5000**

## ğŸ“‹ First Time Setup

1. **Register Root User** (admin account)
2. **Download a Model**:
   - Go to Dashboard
   - Scroll to "Available Models"
   - Click "Download" on any model (recommended: llama3.2:latest - 2GB)
   - Wait for download to complete
3. **Create First Chat**:
   - Click "+ New Chat"
   - Select your downloaded model
   - Start chatting!

## ğŸ¯ Common Tasks

### Download a Model
Dashboard â†’ "Available Models" â†’ Click "Download" â†’ Wait for progress bar

### Delete a Model
Dashboard â†’ "Downloaded Models" â†’ Click "Delete" on model

### Create New Chat
Dashboard â†’ "+ New Chat" â†’ Choose model â†’ Create

### Change Default Model
Click "Settings" â†’ Select model â†’ Save

## ğŸ› ï¸ Troubleshooting

### App won't start?
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it
ollama serve
```

### Can't download models?
```bash
# Restart Ollama
pkill ollama
ollama serve
```

### Models not showing in chat?
- Refresh the browser page
- Make sure model download completed 100%

## ğŸ’¡ Pro Tips

- **Start small**: Download llama3.2 (2GB) first
- **Storage**: Each model takes 2-40GB disk space
- **Performance**: Close other apps for faster responses
- **RAM**: 8GB+ recommended for smooth operation

## ğŸ“± Access from Other Devices

1. Find your computer's IP:
   ```bash
   hostname -I | awk '{print $1}'
   ```

2. Access from phone/tablet:
   ```
   http://YOUR_IP:5000
   ```

## ğŸ”§ Advanced

### Custom Ollama Host
```bash
export OLLAMA_HOST=http://192.168.1.100:11434
python app.py
```

### Docker Deployment
```bash
cd app
docker-compose up -d
```

### Check Ollama Models
```bash
ollama list              # Show downloaded models
ollama pull llama3.2     # Download model manually
ollama rm llama3.2       # Remove model manually
```

---

**Need more help?** See `OLLAMA_SETUP.md` for detailed documentation.

